\documentclass[a4paper,10pt]{article}

\usepackage[table]{xcolor}
\usepackage[ruled]{algorithm2e}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{amsmath,boxedminipage}
\usepackage{amssymb}
\usepackage[totalwidth=166mm,totalheight=240mm]{geometry}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\parindent0mm
%\pagestyle{empty}


\usepackage{tikz}
\usetikzlibrary{arrows}
\newcommand{\bigoh}{\mathcal{O}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\avg}{\textrm{avg}}
\newcommand{\nop}[1]{}
\newcommand{\var}{\textrm{var}}
\newcommand{\E}{\textrm{E}}
\newcounter{aufgc}
\newenvironment{exercise}[1]%
{\refstepcounter{aufgc}\textbf{Exercise \arabic{aufgc}} \emph{#1}\\}
{
	
	\hrulefill\medskip}%

\renewcommand{\labelenumi}{(\alph{enumi})}

%\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}

%\newcommand{\solution}[1]{\bigskip \paragraph{Solution} #1}
\newcommand{\solution}[1]{}

\begin{document}

%% Header
\begin{minipage}[b]{0.58\textwidth}
	%	\centering
	\large
	School of Computer Science and Technology\\University of Science and Technology of China
\end{minipage}

\hrulefill

\vspace{0.2cm}
\begin{center}
	{\large \bf Exercise Sheet 2 for \\[1mm]
		Design and Analysis of Algorithms\\[0.5mm]
		Autumn 2022}\\
	\textcolor{red}{Due 18 Oct 2022 at 23:59}
	\bigskip


\end{center}
\vspace{0.1cm}



\hrulefill\medskip
%% Enter here the exercises !!


\newcommand{\Alg}{\ensuremath{\mathcal{A}}}



\begin{exercise}{}

	Suppose each CPU can execute at most one process at any time. Consider the following experiment, which proceeds in a sequence of rounds. For the first round, we have $n$ processes, which are assigned independently and uniformly at random to $n$ CPUs. For any $i\geq 1$, after round $i$, we first find all the processes $p$ such that $p$ has been assigned to a CPU $C$ \emph{by itself}, i.e., $p$ is the \emph{unique} process that has been assigned to $C$; then we remove all such processes (as they would be executed) in round $i$. The remaining processes are retained for round $i+1$, in which they are assigned independently and uniformly at random to the $n$ CPUs.
	%\vspace{1em}
	\begin{enumerate}
		\item If there are $b$ processes at the start of a round, what is the expected number of processes at the start of the next round?

		      \vspace{0.5em}
		\item Suppose that every round the number of removed processes was exactly the expected number of removed processes. Show that all the processes will be removed in $O(\log\log n)$ rounds.

	\end{enumerate}

	\textit{Hint:} If $x_j$ is the expected number of processes left after $j$ rounds, show and use that $x_{j+1}\leq x_j^2/n$. You can use the fact that $1-kx\leq (1-x)^k$ for $0<x<1$ and $k\leq \frac{1}{x}$.

	\hrulefill

	\textbf{Solution:}
	\begin{enumerate}
		\item $Pr(\text{the ith bin has exactly 1 ball}) = \binom{b}{1}\frac{1}{n}(1-\frac{1}{n})^{(b-1)} = \frac{b}{n}(1-\frac{1}{n})^{(b-1)}$ \\
		      $E(\text{numbers of 1 ball bins}) = \sum_{i=1}^{n} Pr(\text{the ith bin has exactly 1 ball}) = b(1-\frac{1}{n})^{(b-1)}$ \\
		      so, the expected number of processes at the start of the next round is $b - E(\text{numbers of 1 ball bins}) = b-b(1-\frac{1}{n})^{(b-1)}$
		\item If $x_j$ is the expected number of processes left after $j$ rounds, $x_{j+1} = x_{j}(1-(1-\frac{1}{n})^{x_{j}-1}) \le \frac{x_{j} (x_{j}-1)}{n} \le \frac{x_{j}^2}{n}$ \\
		      $x_1 = n(1-(1-\frac{1}{n})^{n-1}) = n(1-\frac{1}{e})$, when $n \to \infty$. Set $k=(1-\frac{1}{e})^{-1} > 1$ which is a small constant. \\
		      So, $x_1=\frac{n}{k}$, $x_2 \le x_1^2/n = n^2/(nk^2) = n/k^2$, $x_3 = n/k^4$, ... , $x_(j+1) = n/k^{2^j} = 1$, which suppose on the $j+1$ ground, all the process are removed. \\
		      $\therefore n=k^{2^j}$, $j = log_2log_kn$, $\therefore j+1 = O(loglogn)$.
	\end{enumerate}

\end{exercise}




\begin{exercise}{}

	Suppose you are given a biased coin that has $\Pr[\text{HEADS}]=p\geq a$, for some fixed $a$, without being given any other information about $p$.
	\begin{enumerate}
		\item Devise a procedure that outputs a value $\tilde{p}$ such that you can guarantee that $\Pr[|p-\tilde{p}|\geq\varepsilon p] \leq \delta$, for any choice of the constants $0<a,\varepsilon,\delta<1$. (The value $\tilde{p}$ is often called the estimate of $p$.)
		\item Let $N$ be the number of times you need to flip the biased coin to obtain the estimate. What is the smallest value of $N$ for which you can still give the above guarantee?
	\end{enumerate}

	%Design an FPT algorithm for this problem, and argue why it is correct.

	\medskip

	\emph{Hint:} flip the coin a few times and consider the fraction of times seeing HEADS.

	\hrulefill

	\textbf{Solution:}
	\begin{enumerate}

		\begin{algorithm}
			\label{coinsalg}
			\caption{COINSALG}
			% \begin{algorithmic}[1] %一行一个标号
			% \STATE Initialization: $X \leftarrow 0$; $N \leftarrow$ a selected number;
			% \FOR{$i \leftarrow 1$ to $N$ do}
			% \STATE Flip the biased coin;
			% \IF{seeing HEADS}
			% \STATE $X \leftarrow X + 1$;
			% \ENDIF
			% \ENDFOR
			% \STATE $\tilde{p} \leftarrow \frac{X}{N}$;
			% \RETURN $\tilde{p}$
			% \end{algorithmic}
		\end{algorithm}
		\item As the Algorithm~\ref{coinsalg}, I devise a procedure that return a value $\tilde{p}$ that can guarantee that $\Pr[|p-\tilde{p}|\geq\varepsilon p] \leq \delta$, for any choice of the constants $0<a,\varepsilon,\delta<1$, and I will prove it as bellow.
		\item Set $X_i = 1 \text{ if and only if at the ith flip time, seeing the HEADS, otherwise } X_i = 0$. $X=\sum_{i=1}^{N}X_i$, $E(\text{number of all HEADS in N times}) = EX = NEX_i = Np$, $\tilde{p}=\frac{X}{N}$ \\
		      $Pr(|p-\tilde{p}|\ge \varepsilon p) = Pr(|p-\frac{X}{N}|\ge \varepsilon p )= Pr(|X-Np|\ge \varepsilon Np) = Pr(|X-EX|\ge \varepsilon EX)$, $\because X_i \in [0,1]$, we can use the Chernoff bound, $\therefore Pr(|X-EX|\ge \varepsilon EX) \le 2exp(\frac{-EX\varepsilon^2}{3}) \le \delta$, $\therefore 2exp(\frac{-Np\varepsilon^2}{3}) \le \delta$, and we can solve the $N$. \\
		      $N \ge \frac{-3ln \delta}{2p\varepsilon^2}$, $\because p\ge a$, we get the smallest $N$ value that can still give the above guarantee is $N=\frac{-3ln \delta}{2a\varepsilon^2}$


	\end{enumerate}



\end{exercise}

\begin{exercise}{}

	Let \(X\) and \(Y\) be finite sets and let \(Y^X\) denote the set of all
	functions from \(X\) to \(Y\). We will think of these functions as ``hash''
	functions. A family \(\mathcal{H} \subseteq Y^X\) is said to be strongly
	\(2\)-universal if the following property holds, with \(h \in \mathcal{H}\)
	picked uniformly at random:
	% -
	\begin{equation*}
		\forall x, x' \in X \ \forall y, y' \in Y \left( x \neq x' \Rightarrow  \Pr_h
			[h(x) = y \wedge h(x') = y'] = \frac{1}{|Y|^2} \right) \,.
	\end{equation*}

	We are give a a stream \(\mathcal{S} \) of elements of \(X\), and
	suppose that \(\mathcal{S}\) contains at most \(s\) distinct elements.
	Let \(\mathcal{H} \subseteq Y^X\) be a strongly \(2\)-universal hash family with
	\(|Y| = c s^2\) for some constant \(c > 0\). Suppose we use a random function
	\(h \in \mathcal{H}\) to hash.

	Prove that the
	probability of a collision (i.e., the event that two distinct elements of
	\(\mathcal{S}\) hash to the same location) is at most \(1 / (2c)\).

	\hrulefill


	\textbf{Solution:}
	\begin{equation*}
		\forall x, x' \in S \ \forall y, y' \in Y \left( x \neq x' \Rightarrow  \Pr_h
			[h(x) = y \wedge h(x') = y'] = \frac{1}{|Y|^2} = \frac{1}{c^2s^4} \right) \,.
	\end{equation*}
	$Pr(\text{the i,jth distinct elements of S hash to the kth location}) = \binom{s}{2} \binom{|Y|}{1}\frac{1}{|Y|^2} = \frac{s(s-1)cs^2}{2c^2s^4} \le \frac{1}{2c}$.

\end{exercise}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
